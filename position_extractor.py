#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import os
from datetime import datetime, timedelta
import imaplib
import email
from email.header import decode_header
from dotenv import load_dotenv
import ssl
import logging
import sys
from collections import defaultdict

load_dotenv()

log_filename = 'fixed_position_extractor_log.txt'
if os.path.exists(log_filename):
    os.remove(log_filename)

logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('fixed_position_extractor')

class FixedPositionExtractor:
    def __init__(self):
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ø–æ—á—Ç–µ
        self.imap_server = os.environ.get('IMAP_SERVER')
        self.imap_port = int(os.environ.get('IMAP_PORT', 143))
        self.imap_user = os.environ.get('IMAP_USER')
        self.imap_password = os.environ.get('IMAP_PASSWORD')
        
        self.names_data = self._load_names_data()
        
        # –£—Å–∏–ª–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –º—É—Å–æ—Ä–∞
        self.garbage_patterns = [
            r'^[–∞-—è—ë]{1,15}–∏—á$|^[–∞-—è—ë]{1,15}–Ω–∞$',  # –æ—Ç—á–µ—Å—Ç–≤–∞
            r'@|mailto:|\.ru|\.com|http|www',        # email/web
            r'\d{4}|\+\d{2}:\d{2}|^\d+$',           # –¥–∞—Ç—ã/—á–∏—Å–ª–∞
            r'^(–æ—Ç|–∫–æ–º—É|re|fwd)[:,\s]|^>+',         # email –ø—Ä–µ—Ñ–∏–∫—Å—ã
            r'–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥–∞|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü–∞|—Å—É–±–±–æ—Ç–∞|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ',
            r'—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è'
        ]
        
        logger.info("‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üë• –ó–∞–≥—Ä—É–∂–µ–Ω–æ –§–ò–û: {len(self.names_data)}")
        logger.info("üéØ –ù–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏")
    
    def _load_names_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –§–ò–û –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞"""
        names = []
        try:
            with open('name_extractor_log.txt', 'r', encoding='utf-8') as f:
                content = f.read()
            
            name_pattern = r'^\s*\d+\.\s+([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*)\s*\([^)]+\)$'
            
            lines = content.split('\n')
            for line in lines:
                match = re.match(name_pattern, line.strip())
                if match:
                    name = match.group(1).strip()
                    if name not in names:
                        names.append(name)
            
            return names
        except FileNotFoundError:
            logger.warning("‚ö†Ô∏è –§–∞–π–ª name_extractor_log.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    
    def find_complete_positions_for_names(self, text, email_subject="", email_date="", from_addr=""):
        """–ö–ê–†–î–ò–ù–ê–õ–¨–ù–û –ù–û–í–´–ô –ø–æ–∏—Å–∫ –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π"""
        results = []
        
        for name in self.names_data:
            positions = self._extract_complete_positions_near_name(text, name)
            
            for position_info in positions:
                results.append({
                    'name': name,
                    'position': position_info['position'],
                    'confidence': position_info['confidence'],
                    'method': position_info['method'],
                    'context': position_info['context'][:200],
                    'email_subject': email_subject,
                    'email_date': email_date,
                    'from_addr': from_addr
                })
        
        return results
    
    def _extract_complete_positions_near_name(self, text, name):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ü–û–õ–ù–´–• –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π —Ä—è–¥–æ–º —Å –§–ò–û"""
        positions_found = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏–º–µ–Ω–∏
        name_lower = name.lower()
        text_lines = text.split('\n')
        
        for i, line in enumerate(text_lines):
            line_lower = line.lower()
            
            if name_lower in line_lower:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏
                context_lines = []
                
                # –ë–µ—Ä–µ–º ¬±3 —Å—Ç—Ä–æ–∫–∏ –≤–æ–∫—Ä—É–≥ –∏–º–µ–Ω–∏
                start_idx = max(0, i - 3)
                end_idx = min(len(text_lines), i + 4)
                context_lines = text_lines[start_idx:end_idx]
                context_text = '\n'.join(context_lines)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
                methods = [
                    self._method_signature_block_analysis(context_text, name, i - start_idx),
                    self._method_multiline_job_assembly(context_text, name, i - start_idx),
                    self._method_contextual_expansion(context_text, name, i - start_idx)
                ]
                
                # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                for method_result in methods:
                    if method_result and self._is_complete_valid_position(method_result['position']):
                        positions_found.append(method_result)
                        break
        
        return positions_found
    
    def _method_signature_block_analysis(self, context, name, name_line_idx):
        """–ê–Ω–∞–ª–∏–∑ –±–ª–æ–∫–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π —Å –ø–æ–∏—Å–∫–æ–º –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π"""
        
        lines = context.split('\n')
        name_line = lines[name_line_idx] if name_line_idx < len(lines) else ""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 1: –° —É–≤–∞–∂–µ–Ω–∏–µ–º, –§–ò–û, –î–û–õ–ñ–ù–û–°–¢–¨ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        if '—Å —É–≤–∞–∂–µ–Ω–∏–µ–º' in context.lower():
            for i in range(name_line_idx + 1, min(len(lines), name_line_idx + 4)):
                if i < len(lines):
                    candidate = lines[i].strip()
                    if candidate and self._looks_like_complete_job_title(candidate):
                        return {
                            'position': self._clean_and_complete_position(candidate),
                            'confidence': 0.95,
                            'method': 'signature_block_post_name',
                            'context': context
                        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 2: –î–û–õ–ñ–ù–û–°–¢–¨ –ø–µ—Ä–µ–¥ –§–ò–û
        if name_line_idx > 0:
            for i in range(max(0, name_line_idx - 3), name_line_idx):
                candidate = lines[i].strip()
                if candidate and self._looks_like_complete_job_title(candidate):
                    return {
                        'position': self._clean_and_complete_position(candidate),
                        'confidence': 0.90,
                        'method': 'signature_block_pre_name',
                        'context': context
                    }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 3: –î–û–õ–ñ–ù–û–°–¢–¨ | –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è (–≤ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–µ —á—Ç–æ –∏ –§–ò–û)
        if '|' in name_line:
            parts = name_line.split('|')
            for part in parts:
                if name.lower() not in part.lower():
                    candidate = part.strip()
                    if self._looks_like_complete_job_title(candidate):
                        return {
                            'position': self._clean_and_complete_position(candidate),
                            'confidence': 0.85,
                            'method': 'signature_inline_pipe',
                            'context': context
                        }
        
        return None
    
    def _method_multiline_job_assembly(self, context, name, name_line_idx):
        """–°–±–æ—Ä–∫–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π"""
        
        lines = [line.strip() for line in context.split('\n') if line.strip()]
        
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç—è–º–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
        job_candidates = []
        
        for i, line in enumerate(lines):
            if i != name_line_idx and self._could_be_job_part(line):
                job_candidates.append((i, line))
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç—ã, –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
        if job_candidates:
            # –ù–∞—Ö–æ–¥–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –±–ª–∏–∑–∫–∏—Ö –∫ –∏–º–µ–Ω–∏
            close_candidates = [
                (idx, line) for idx, line in job_candidates 
                if abs(idx - name_line_idx) <= 2
            ]
            
            if close_candidates:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ –∏–º–µ–Ω–∏
                close_candidates.sort(key=lambda x: abs(x[0] - name_line_idx))
                
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
                assembled_position = self._assemble_complete_job_title(close_candidates, name_line_idx)
                
                if assembled_position:
                    return {
                        'position': assembled_position,
                        'confidence': 0.80,
                        'method': 'multiline_assembly',
                        'context': context
                    }
        
        return None
    
    def _method_contextual_expansion(self, context, name, name_line_idx):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–æ –ø–æ–ª–Ω—ã—Ö"""
        
        # –ò—â–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        job_indicators = [
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', 
            '–Ω–∞—á–∞–ª—å–Ω–∏–∫', '–∑–∞–≤–µ–¥—É—é—â–∏–π', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä'
        ]
        
        context_lower = context.lower()
        
        for indicator in job_indicators:
            if indicator in context_lower:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–ª–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å —Å —ç—Ç–∏–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º
                expanded = self._expand_job_contextually(context, indicator, name)
                
                if expanded and len(expanded) > len(indicator) + 5:  # –î–æ–ª–∂–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞
                    return {
                        'position': expanded,
                        'confidence': 0.75,
                        'method': 'contextual_expansion',
                        'context': context
                    }
        
        return None
    
    def _expand_job_contextually(self, context, base_indicator, name):
        """–£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –¥–æ –ø–æ–ª–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
        context_lower = context.lower()
        base_pos = context_lower.find(base_indicator.lower())
        
        if base_pos == -1:
            return None
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ–ª–∂–Ω–æ—Å—Ç—å –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã
        start = base_pos
        end = base_pos + len(base_indicator)
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –≤–ª–µ–≤–æ (–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã)
        left_modifiers = [
            '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π', '–≤–µ–¥—É—â–∏–π', '—Å—Ç–∞—Ä—à–∏–π', '–≥–ª–∞–≤–Ω—ã–π', '–∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å', 
            '–∑–∞–º.', '–∏—Å–ø–æ–ª–Ω—è—é—â–∏–π', '–ø–µ—Ä–≤—ã–π', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π'
        ]
        
        expanded_left = self._expand_left(context, start, left_modifiers)
        if expanded_left is not None:
            start = expanded_left
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –≤–ø—Ä–∞–≤–æ (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è)
        right_extensions = [
            '–ø–æ', '–æ—Ç–¥–µ–ª–∞', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞', '–≥—Ä—É–ø–ø—ã', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '—Å–Ω–∞–±–∂–µ–Ω–∏—è',
            '–ø—Ä–æ–¥–∞–∂', '—Ä–∞–∑–≤–∏—Ç–∏—è', '–ø–æ—Å—Ç–∞–≤–æ–∫', '–∑–∞–∫—É–ø–æ–∫', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è', '–ø—Ä–æ–µ–∫—Ç–∞–º',
            '—Ç–µ–Ω–¥–µ—Ä–∞–º', '–∫–ª–∏–µ–Ω—Ç–∞–º–∏', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π', '–º–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏–∏', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π'
        ]
        
        expanded_right = self._expand_right(context, end, right_extensions)
        if expanded_right is not None:
            end = expanded_right
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
        expanded = context[start:end].strip()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ—Å—Ç—å
        expanded = self._handle_multiline_in_expansion(context, start, end)
        
        return self._clean_and_complete_position(expanded)
    
    def _expand_left(self, context, start_pos, modifiers):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–ª–µ–≤–æ —Å –ø–æ–∏—Å–∫–æ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        
        original_start = start_pos
        
        # –ò—â–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø–µ—Ä–µ–¥ –¥–æ–ª–∂–Ω–æ—Å—Ç—å—é
        for modifier in modifiers:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª–µ–≤–∞ –æ—Ç —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
            search_start = max(0, start_pos - 50)  # –ò—â–µ–º –≤ —Ä–∞–¥–∏—É—Å–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
            left_context = context[search_start:start_pos].lower()
            
            modifier_pos = left_context.rfind(modifier)
            if modifier_pos != -1:
                # –ù–∞—à–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Å–¥–≤–∏–≥–∞–µ–º –Ω–∞—á–∞–ª–æ
                new_start = search_start + modifier_pos
                if new_start < original_start:
                    return new_start
        
        return None
    
    def _expand_right(self, context, end_pos, extensions):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–ø—Ä–∞–≤–æ —Å –ø–æ–∏—Å–∫–æ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π"""
        
        original_end = end_pos
        current_pos = end_pos
        
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–∫–∞ –Ω–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        while current_pos < len(context):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
            while current_pos < len(context) and context[current_pos].isspace():
                current_pos += 1
            
            if current_pos >= len(context):
                break
            
            # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
            word_start = current_pos
            while current_pos < len(context) and (context[current_pos].isalpha() or context[current_pos] in '-'):
                current_pos += 1
            
            if word_start < current_pos:
                word = context[word_start:current_pos].lower()
                
                if word in extensions:
                    # –ù–∞—à–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                    continue
                else:
                    # –ù–µ –Ω–∞—à–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                    break
            else:
                break
        
        return current_pos if current_pos > original_end else None
    
    def _handle_multiline_in_expansion(self, context, start, end):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏"""
        
        lines = context[start:end].split('\n')
        
        # –ï—Å–ª–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–∞—è, —Å–æ–±–∏—Ä–∞–µ–º –µ—ë –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        if len(lines) > 1:
            job_parts = []
            for line in lines:
                cleaned_line = line.strip()
                if cleaned_line and not self._is_garbage_line(cleaned_line):
                    job_parts.append(cleaned_line)
            
            if job_parts:
                return ' '.join(job_parts)
        
        return context[start:end].strip()
    
    def _assemble_complete_job_title(self, candidates, name_line_idx):
        """–°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏–∑ —á–∞—Å—Ç–µ–π"""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏–º–µ–Ω–∏
        candidates.sort(key=lambda x: x[0])
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–±—Ä–∞—Ç—å –ª–æ–≥–∏—á–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
        job_parts = []
        
        for idx, line in candidates:
            cleaned = line.strip()
            if cleaned and not self._is_garbage_line(cleaned):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ª–æ–≥–∏—á–Ω–æ –ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç—É —á–∞—Å—Ç—å
                if self._is_logical_job_part(cleaned, job_parts):
                    job_parts.append(cleaned)
        
        if job_parts:
            assembled = ' '.join(job_parts)
            return self._clean_and_complete_position(assembled)
        
        return None
    
    def _could_be_job_part(self, line):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –±—ã—Ç—å —á–∞—Å—Ç—å—é –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        if not line or len(line.strip()) < 3:
            return False
        
        line_lower = line.lower().strip()
        
        # –°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        job_keywords = [
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–Ω–∞—á–∞–ª—å–Ω–∏–∫',
            '–∑–∞–≤–µ–¥—É—é—â–∏–π', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—ç–∫—Å–ø–µ—Ä—Ç',
            '–ø–æ', '–æ—Ç–¥–µ–ª–∞', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞', '–≥—Ä—É–ø–ø—ã', '—Å–Ω–∞–±–∂–µ–Ω–∏—è', '–ø—Ä–æ–¥–∞–∂'
        ]
        
        return any(keyword in line_lower for keyword in job_keywords)
    
    def _is_logical_job_part(self, part, existing_parts):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        
        part_lower = part.lower()
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å, –ø—Ä–∏–Ω–∏–º–∞–µ–º –ª—é–±—É—é —Ä–∞–∑—É–º–Ω—É—é
        if not existing_parts:
            return self._looks_like_complete_job_title(part)
        
        # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å —á–∞—Å—Ç–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        combined_lower = ' '.join(existing_parts).lower()
        
        # –õ–æ–≥–∏—á–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
        logical_continuations = [
            ('–º–µ–Ω–µ–¥–∂–µ—Ä', ['–ø–æ']),
            ('—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', ['–ø–æ']),
            ('–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å', ['–≤', '–ø–æ']),
            ('—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', ['–æ—Ç–¥–µ–ª–∞', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞', '–≥—Ä—É–ø–ø—ã']),
            ('–Ω–∞—á–∞–ª—å–Ω–∏–∫', ['–æ—Ç–¥–µ–ª–∞'])
        ]
        
        for base, continuations in logical_continuations:
            if base in combined_lower:
                if any(cont in part_lower for cont in continuations):
                    return True
        
        return False
    
    def _looks_like_complete_job_title(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—ã–≥–ª—è–¥–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∞–∫ –ø–æ–ª–Ω–∞—è –¥–æ–ª–∂–Ω–æ—Å—Ç—å"""
        if not text or len(text.strip()) < 5:
            return False
        
        text_lower = text.lower().strip()
        
        # –ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º—É—Å–æ—Ä–∞
        if self._is_garbage_line(text):
            return False
        
        # –î–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
        job_indicators = [
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–Ω–∞—á–∞–ª—å–Ω–∏–∫',
            '–∑–∞–≤–µ–¥—É—é—â–∏–π', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '–≤—Ä–∞—á'
        ]
        
        has_job_indicator = any(indicator in text_lower for indicator in job_indicators)
        
        # –ò–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–≤—è–∑–∫–∏
        job_patterns = [
            r'–ø–æ\s+\w+',  # "–ø–æ –ø—Ä–æ–¥–∞–∂–∞–º", "–ø–æ –∑–∞–∫—É–ø–∫–∞–º"
            r'–æ—Ç–¥–µ–ª–∞\s+\w+',  # "–æ—Ç–¥–µ–ª–∞ —Å–Ω–∞–±–∂–µ–Ω–∏—è"
            r'–≤\s+[–ê-–Ø–Å]{2,4}',  # "–≤ –°–§–û"
        ]
        
        has_job_pattern = any(re.search(pattern, text) for pattern in job_patterns)
        
        return has_job_indicator or has_job_pattern
    
    def _is_complete_valid_position(self, position):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –ø–æ–ª–Ω–æ–π –∏ –≤–∞–ª–∏–¥–Ω–æ–π"""
        if not position or len(position.strip()) < 8:  # –ú–∏–Ω–∏–º—É–º 8 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            return False
        
        position_lower = position.lower().strip()
        
        # –ù–µ –¥–æ–ª–∂–Ω–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –ø—Ä–µ–¥–ª–æ–≥–∞–º–∏ (–ø—Ä–∏–∑–Ω–∞–∫ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ—Å—Ç–∏)
        ending_prepositions = ['–ø–æ', '–≤', '–¥–ª—è', '–æ—Ç', '–∫', '–Ω–∞', '—Å', '—É']
        words = position_lower.split()
        if words and words[-1] in ending_prepositions:
            return False
        
        # –ù–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º—É—Å–æ—Ä–æ–º
        if self._is_garbage_line(position):
            return False
        
        # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        return self._looks_like_complete_job_title(position)
    
    def _is_garbage_line(self, line):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –º—É—Å–æ—Ä–æ–º"""
        if not line:
            return True
        
        line_clean = line.strip().lower()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞
        for pattern in self.garbage_patterns:
            if re.search(pattern, line_clean):
                return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if len(line_clean) < 3:
            return True
        
        if line_clean in ['—Å —É–≤–∞–∂–µ–Ω–∏–µ–º', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', '—Å–ø–∞—Å–∏–±–æ']:
            return True
        
        return False
    
    def _clean_and_complete_position(self, raw_position):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        if not raw_position:
            return None
        
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        cleaned = raw_position.strip()
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        cleaned = re.sub(r'^[^\w\u0400-\u04FF]+', '', cleaned)
        cleaned = re.sub(r'[^\w\u0400-\u04FF\s\-()¬´¬ª""]+$', '', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–º–ª—è—é—â–∏–µ –∫–∞–≤—ã—á–∫–∏
        for quote_pair in [('"', '"'), ('¬´', '¬ª'), ('‚Äû', '"')]:
            if cleaned.startswith(quote_pair[0]) and cleaned.endswith(quote_pair[1]):
                cleaned = cleaned[1:-1].strip()
        
        # Capitalize first letter if needed
        if cleaned and cleaned[0].islower():
            cleaned = cleaned[0].upper() + cleaned[1:]
        
        return cleaned.strip()
    
    def smart_deduplicate_results(self, raw_results):
        """–£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π"""
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∏–º–µ–Ω–∞–º
        name_groups = defaultdict(list)
        
        for result in raw_results:
            normalized_name = self._normalize_name_for_grouping(result['name'])
            name_groups[normalized_name].append(result)
        
        deduplicated = []
        
        for normalized_name, group_results in name_groups.items():
            # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –∏–º–µ–Ω –Ω–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            position_groups = defaultdict(list)
            
            for result in group_results:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ–ª–∂–Ω–æ—Å—Ç—å –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
                normalized_pos = self._normalize_position_for_grouping(result['position'])
                position_groups[normalized_pos].append(result)
            
            # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            for normalized_pos, pos_results in position_groups.items():
                # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π confidence –∏ —Å–∞–º–æ–π –ø–æ–ª–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç—å—é
                best_result = max(pos_results, key=lambda x: (x['confidence'], len(x['position'])))
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–µ –ø–æ–ª–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–º–µ–Ω–∏ –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
                best_result['name'] = max([r['name'] for r in pos_results], key=len)
                best_result['position'] = max([r['position'] for r in pos_results], key=len)
                
                deduplicated.append(best_result)
        
        return sorted(deduplicated, key=lambda x: x['name'])
    
    def _normalize_name_for_grouping(self, name):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = re.sub(r'\s+', ' ', name.lower().strip())
        
        # –î–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–∞ —Å–ª–æ–≤–∞ (—Ñ–∞–º–∏–ª–∏—è + –∏–º—è)
        words = normalized.split()
        if len(words) >= 2:
            return f"{words[0]} {words[1]}"
        return normalized
    
    def _normalize_position_for_grouping(self, position):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        normalized = position.lower().strip()
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        replacements = {
            '–∑–∞–º.': '–∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å',
            '–∑–∞–≤.': '–∑–∞–≤–µ–¥—É—é—â–∏–π',
            '—Å—Ñ–æ': '–°–§–û',
            '–∫–¥–ª': '–ö–î–õ'
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized
    
    def _decode_header_clean(self, header_str):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ—á—Ç—ã"""
        if not header_str:
            return ""
        try:
            from email.header import decode_header, make_header
            decoded = str(make_header(decode_header(header_str)))
            return decoded
        except:
            return header_str
    
    def _parse_email_date(self, date_str):
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –ø–∏—Å—å–º–∞"""
        if not date_str:
            return "–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"
        try:
            dt = email.utils.parsedate_to_datetime(date_str)
            dt_adjusted = dt.replace(tzinfo=None) + timedelta(hours=4)
            return dt_adjusted.strftime('%d.%m.%Y %H:%M')
        except Exception:
            return date_str[:16] if len(date_str) > 16 else date_str
    
    def _extract_email_body_fast(self, msg):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–∞ –ø–∏—Å—å–º–∞"""
        body = ""
        max_size = 15000
        
        try:
            if msg.is_multipart():
                for part in msg.walk():
                    if part.get_content_type() == "text/plain":
                        charset = part.get_content_charset() or 'utf-8'
                        try:
                            payload = part.get_payload(decode=True)
                            if payload:
                                content = payload.decode(charset, errors="ignore")
                                body = content[:max_size]
                                break
                        except Exception:
                            continue
            else:
                charset = msg.get_content_charset() or 'utf-8'
                try:
                    payload = msg.get_payload(decode=True)
                    if payload:
                        content = payload.decode(charset, errors="ignore")
                        body = content[:max_size]
                except Exception:
                    body = ""
        except Exception:
            body = ""
        
        return body.strip()
    
    def test_single_date_detailed(self, date_str):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –¥–∞—Ç—ã"""
        logger.info("=" * 80)
        logger.info(f"üìã –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –î–û–õ–ñ–ù–û–°–¢–ï–ô –ó–ê {date_str}")
        logger.info("=" * 80)
        
        try:
            logger.info(f"üîå –ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ {self.imap_server}...")
            
            mailbox = imaplib.IMAP4(self.imap_server, self.imap_port)
            mailbox.starttls(ssl.create_default_context())
            mailbox.login(self.imap_user, self.imap_password)
            mailbox.select('INBOX')
            
            dt = datetime.strptime(date_str, '%Y-%m-%d')
            imap_date = dt.strftime('%d-%b-%Y')
            criteria = f'(ON "{imap_date}")'
            
            status, data = mailbox.search(None, criteria)
            mail_ids = data[0].split() if status == 'OK' else []
            
            total_emails = len(mail_ids)
            logger.info(f"üì¨ –í—Å–µ–≥–æ –ø–∏—Å–µ–º –∑–∞ {date_str}: {total_emails}")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            if total_emails == 0:
                logger.info("‚ùå –ü–∏—Å–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
                mailbox.logout()
                return {
                    'date': date_str,
                    'total_emails': 0,
                    'emails_processed': 0,
                    'final_results': [],  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª—é—á
                    'raw_count': 0,
                    'deduplicated_count': 0
                }
            
            all_results = []
            emails_processed = 0
            
            for i, mail_id in enumerate(mail_ids, 1):
                try:
                    status, msg_data = mailbox.fetch(mail_id, '(RFC822)')
                    if status != 'OK':
                        continue
                    
                    raw_email = msg_data[0][1]
                    msg = email.message_from_bytes(raw_email)
                    
                    subject = self._decode_header_clean(msg.get('Subject', '–ë–µ–∑ —Ç–µ–º—ã'))
                    from_addr = self._decode_header_clean(msg.get('From', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                    email_date = self._parse_email_date(msg.get('Date', ''))
                    
                    body = self._extract_email_body_fast(msg)
                    
                    # –ù–û–í–´–ô –£–õ–£–ß–®–ï–ù–ù–´–ô –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞
                    email_results = self.find_complete_positions_for_names(body, subject, email_date, from_addr)
                    
                    if email_results:
                        all_results.extend(email_results)
                        emails_processed += 1
                        
                        logger.info(f"\nüìß –ü–∏—Å—å–º–æ {i}/{total_emails}: {email_date}")
                        logger.info(f"   üìù –¢–µ–º–∞: {subject[:60]}...")
                        logger.info(f"   üë§ –û—Ç: {from_addr[:50]}...")
                        logger.info("   üìù –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ü–û–õ–ù–´–ï –¥–æ–ª–∂–Ω–æ—Å—Ç–∏:")
                        
                        for result in email_results:
                            logger.info(f"   ‚úÖ {result['name']}")
                            logger.info(f"    ‚ñ∂Ô∏è {result['position']}")
                            logger.info(f"       üìä {result['confidence']:.2f} | {result['method']}")
                
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∏—Å—å–º–∞ {i}: {e}")
                    continue
            
            mailbox.logout()
            
            # –£–º–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            logger.info(f"\nüîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é...")
            logger.info(f"   üìä –°—ã—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results)}")
            
            final_results = self.smart_deduplicate_results(all_results)
            logger.info(f"   üìä –§–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(final_results)}")
            
            logger.info("=" * 80)
            logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ó–ê {date_str}")
            logger.info("=" * 80)
            logger.info(f"üì¨ –í—Å–µ–≥–æ –ø–∏—Å–µ–º: {total_emails}")
            logger.info(f"üìã –ü–∏—Å–µ–º —Å –¥–æ–ª–∂–Ω–æ—Å—Ç—è–º–∏: {emails_processed}")
            logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –§–ò–û+–î–æ–ª–∂–Ω–æ—Å—Ç—å: {len(final_results)}")
            
            if final_results:
                logger.info(f"\nüìã –§–ò–ù–ê–õ–¨–ù–´–ï –ü–û–õ–ù–´–ï –î–û–õ–ñ–ù–û–°–¢–ò –ó–ê {date_str}:")
                for i, result in enumerate(final_results, 1):
                    logger.info(f"   {i:2d}. {result['name']} ‚Äî {result['position']}")
                    logger.info(f"       üìä {result['confidence']:.2f} | {result['method']}")
            
            logger.info("=" * 80)
            
            return {
                'date': date_str,
                'total_emails': total_emails,
                'emails_processed': emails_processed,
                'final_results': final_results,  # ‚Üê –í—Å–µ–≥–¥–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                'raw_count': len(all_results),
                'deduplicated_count': len(final_results)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {date_str}: {e}")
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                'date': date_str,
                'total_emails': 0,
                'emails_processed': 0,
                'final_results': [],
                'raw_count': 0,
                'deduplicated_count': 0
            }
    
    def test_date_range_detailed(self, start_date, end_date):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç"""
        logger.info("=" * 80)
        logger.info(f"üöÄ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –ü–û–õ–ù–´–• –î–û–õ–ñ–ù–û–°–¢–ï–ô: {start_date} - {end_date}")
        logger.info("üí° –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –ø–æ–ª–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ + —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ KeyError")
        logger.info("=" * 80)
        
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        current_date = start_dt
        
        all_final_results = []
        all_daily_results = []
        total_emails_all = 0
        total_days = (end_dt - start_dt).days + 1
        
        day_counter = 1
        while current_date <= end_dt:
            date_str = current_date.strftime('%Y-%m-%d')
            
            logger.info(f"\nüéØ –î–ï–ù–¨ {day_counter}/{total_days}: {date_str}")
            logger.info("=" * 50)
            
            daily_results = self.test_single_date_detailed(date_str)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º daily_results –∏ –Ω–∞–ª–∏—á–∏–µ final_results
            if daily_results:
                total_emails_all += daily_results.get('total_emails', 0)
                final_results = daily_results.get('final_results', [])
                if final_results:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    all_final_results.extend(final_results)
                all_daily_results.append(daily_results)
            
            current_date += timedelta(days=1)
            day_counter += 1
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –≤—Å–µ–º—É –ø–µ—Ä–∏–æ–¥—É
        if all_final_results:
            logger.info(f"\nüîÑ –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –≤—Å–µ–º—É –ø–µ—Ä–∏–æ–¥—É...")
            period_final = self.smart_deduplicate_results(all_final_results)
        else:
            period_final = []
        
        logger.info("=" * 80)
        logger.info(f"üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ {start_date} - {end_date}")
        logger.info("=" * 80)
        logger.info(f"üìÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–Ω–µ–π: {total_days}")
        logger.info(f"üì¨ –í—Å–µ–≥–æ –ø–∏—Å–µ–º: {total_emails_all}")
        logger.info(f"üéØ –ò—Ç–æ–≥–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –§–ò–û+–î–æ–ª–∂–Ω–æ—Å—Ç—å: {len(period_final)}")
        
        if period_final:
            logger.info(f"\nüìã –ò–¢–û–ì–û–í–´–ï –ü–û–õ–ù–´–ï –î–û–õ–ñ–ù–û–°–¢–ò –ó–ê –ü–ï–†–ò–û–î:")
            for i, result in enumerate(period_final, 1):
                logger.info(f"   {i:3d}. {result['name']} ‚Äî {result['position']}")
                logger.info(f"        üìä confidence: {result['confidence']:.2f} | method: {result['method']}")
        
        logger.info("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–ù–Ø–ú:")
        for day_result in all_daily_results:
            final_count = len(day_result.get('final_results', []))
            logger.info(f"   üìÖ {day_result['date']}: {day_result.get('total_emails', 0)} –ø–∏—Å–µ–º ‚Üí {final_count} –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π")
        
        logger.info("=" * 80)
        logger.info("‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –ü–û–õ–ù–´–• –î–û–õ–ñ–ù–û–°–¢–ï–ô –ó–ê–í–ï–†–®–ï–ù")
        logger.info("=" * 80)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open('fixed_position_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'period': f"{start_date} - {end_date}",
                'total_days': total_days,
                'total_emails': total_emails_all,
                'final_unique_pairs': len(period_final),
                'results': period_final,
                'daily_summary': all_daily_results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ fixed_position_results.json")
        
        return period_final

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û –≠–ö–°–¢–†–ê–ö–¢–û–†–ê –ü–û–õ–ù–´–• –î–û–õ–ñ–ù–û–°–¢–ï–ô v4.0")
    logger.info("üí° –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: KeyError —É—Å—Ç—Ä–∞–Ω–µ–Ω + –ø–æ–∏—Å–∫ –ü–û–õ–ù–´–• –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π!")
    
    extractor = FixedPositionExtractor()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞—Ç
    start_date = '2025-07-29'
    end_date = '2025-08-04'
    
    results = extractor.test_date_range_detailed(start_date, end_date)
    
    logger.info(f"\nüéâ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –ó–ê–í–ï–†–®–ï–ù!")
    logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π: {len(results) if results else 0}")

if __name__ == "__main__":
    main()
