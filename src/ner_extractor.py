from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    PER,
    ORG,
    LOC,
    Doc
)
import re
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class NERResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    persons: List[str] = None
    organizations: List[str] = None
    locations: List[str] = None
    positions: List[str] = None
    
    def __post_init__(self):
        if self.persons is None:
            self.persons = []
        if self.organizations is None:
            self.organizations = []
        if self.locations is None:
            self.locations = []
        if self.positions is None:
            self.positions = []

class RussianNERExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å Natasha"""
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words_person = self._load_stop_words('data/stop_words_person.txt')
        self.stop_words_org = self._load_stop_words('data/stop_words_org.txt')
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π)
        self.position_keywords = [
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–¥–∏—Ä–µ–∫—Ç—Ä–∏—Å–∞', '–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–≤—Ä–∞—á', '–∏–Ω–∂–µ–Ω–µ—Ä',
            '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–Ω–∞—á–∞–ª—å–Ω–∏–∫', '–Ω–∞—á–∞–ª—å–Ω–∏—Ü–∞', '–∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å', '–∑–∞–≤–µ–¥—É—é—â–∏–π', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä'
        ]
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        self.position_patterns = [
            # –î–∏—Ä–µ–∫—Ç–æ—Ä —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
            r'–î–∏—Ä–µ–∫—Ç–æ—Ä(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*(?:\s+—Ü–µ–Ω—Ç—Ä–∞|\s+–æ—Ç–¥–µ–ª–∞|\s+–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞|\s+—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è|\s+—Å–ª—É–∂–±—ã|\s+—Ñ–∏–ª–∏–∞–ª–∞)*(?:\s+[–∞-—è—ë\s]+)?',
            # –ú–µ–Ω–µ–¥–∂–µ—Ä —Å "–ø–æ" –∏ –±–µ–∑
            r'–ú–µ–Ω–µ–¥–∂–µ—Ä(?:\s+[–∞-—è—ë]+)*(?:\s+–ø–æ\s+[–∞-—è—ë\s]+)?',
            # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Å "–ø–æ" –∏ –±–µ–∑  
            r'–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç(?:\s+[–∞-—è—ë]+)*(?:\s+–ø–æ\s+[–∞-—è—ë\s]+)?',
            # –í—Ä–∞—á —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
            r'–í—Ä–∞—á(?:\s+–ø–æ\s+[–∞-—è—ë\s]+)?(?:\s+–æ—Ç–¥–µ–ª–µ–Ω–∏—è\s+[–∞-—è—ë\s]+)?(?:\s+–º–µ—Ç—Ä–æ–ª–æ–≥–∏–∏)?(?:\s+–∏)?(?:\s+—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏)?',
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            r'–ò–Ω–∂–µ–Ω–µ—Ä(?:\s+[–∞-—è—ë\s]+)?',
            r'–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å(?:\s+[–∞-—è—ë\s]+)?',
            r'–ù–∞—á–∞–ª—å–Ω–∏–∫(?:\s+[–∞-—è—ë\s]+)?',
            r'–ó–∞–≤–µ–¥—É—é—â–∏–π(?:\s+[–∞-—è—ë\s]+)?',
            r'–ó–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å(?:\s+[–∞-—è—ë\s]+)?'
        ]

    def _load_stop_words(self, filename: str) -> set:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filename, encoding='utf-8') as f:
                stop_words = set()
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        stop_words.add(line.lower())
                return stop_words
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return set()

    def _clean_fio_text(self, fio_text: str) -> str:
        """–¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –§–ò–û –æ—Ç –º—É—Å–æ—Ä–∞"""
        if not fio_text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
        cleaned = re.sub(r'[\r\n\t]+', ' ', fio_text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –§–ò–û
        suspicious_patterns = [
            r'\b(—Ç–µ–ª|—Ç–µ–ª–µ—Ñ–æ–Ω|–º–æ–±|–º–æ–±–∏–ª—å–Ω—ã–π|—Ñ–∞–∫—Å|email|e-mail)\b',
            r'\b(–¥–∏—Ä–µ–∫—Ç–æ—Ä|–º–µ–Ω–µ–¥–∂–µ—Ä|—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç|–≤—Ä–∞—á)\b',
            r'\b(–æ–æ–æ|–∑–∞–æ|–∞–æ|–∏–ø)\b'
        ]
        
        for pattern in suspicious_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        cleaned = re.sub(r'^[^\w]+|[^\w]+$', '', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã —Å–Ω–æ–≤–∞
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Ö–æ–∂ –Ω–∞ –§–ò–û (2-3 —Å–ª–æ–≤–∞, –∫–∞–∂–¥–æ–µ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π)
        words = cleaned.split()
        if len(words) < 2 or len(words) > 4:
            return ""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
        for word in words:
            if not re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+$', word):
                return ""
        
        return cleaned

    def _is_stop_word_person(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º –¥–ª—è –ø–µ—Ä—Å–æ–Ω"""
        text_lower = text.lower().strip()
        
        if text_lower in self.stop_words_person:
            return True
        
        for stop_word in self.stop_words_person:
            if len(stop_word) > 3 and stop_word in text_lower:
                return True
        
        return False

    def _is_stop_word_org(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        text_lower = text.lower().strip()
        
        if text_lower in self.stop_words_org:
            return True
        
        for stop_word in self.stop_words_org:
            if len(stop_word) > 3 and stop_word in text_lower:
                return True
        
        return False

    def _is_position_not_organization(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç—å—é (–ù–ï –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π)"""
        text_lower = text.lower().strip()
        
        # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ - —ç—Ç–æ –¥–æ–ª–∂–Ω–æ—Å—Ç—å
        for keyword in self.position_keywords:
            if text_lower.startswith(keyword):
                return True
        
        # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        for pattern in self.position_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        return False

    def _extend_org_with_region(self, org: str, line: str) -> str:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
        regional_patterns = [
            # "–≤ [–ù–∞–∑–≤–∞–Ω–∏–µ] –∫—Ä–∞–µ"
            rf"{re.escape(org)}\s+–≤\s+[–ê-–Ø–Å][–∞-—è—ë\-]+(?:\s+[–∞-—è—ë\-]+)?\s+–∫—Ä–∞–µ",
            # "–≤ [–ù–∞–∑–≤–∞–Ω–∏–µ] –æ–±–ª–∞—Å—Ç–∏"
            rf"{re.escape(org)}\s+–≤\s+[–ê-–Ø–Å][–∞-—è—ë\-]+(?:\s+[–∞-—è—ë\-]+)?\s+–æ–±–ª–∞—Å—Ç–∏",
            # "–≤ [–ù–∞–∑–≤–∞–Ω–∏–µ] –æ–±–ª."
            rf"{re.escape(org)}\s+–≤\s+[–ê-–Ø–Å][–∞-—è—ë\-]+(?:\s+[–∞-—è—ë\-]+)?\s+–æ–±–ª\.",
            # "–≤ –†–µ—Å–ø—É–±–ª–∏–∫–µ [–ù–∞–∑–≤–∞–Ω–∏–µ]"
            rf"{re.escape(org)}\s+–≤\s+–†–µ—Å–ø—É–±–ª–∏–∫–µ\s+[–ê-–Ø–Å][–∞-—è—ë\-]+",
            # "–≤ [–ù–∞–∑–≤–∞–Ω–∏–µ] —Ä–µ—Å–ø—É–±–ª–∏–∫–µ"
            rf"{re.escape(org)}\s+–≤\s+[–ê-–Ø–Å][–∞-—è—ë\-]+\s+—Ä–µ—Å–ø—É–±–ª–∏–∫–µ",
        ]
        
        for pattern in regional_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        
        return line.strip()

    def clean_organization_text(self, text: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—é, —É–¥–∞–ª—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ—Ä–æ–¥–∞ –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        cleaned = re.sub(r'\s+', ' ', text).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –¥–æ–ª–∂–Ω–æ—Å—Ç—å—é
        if self._is_position_not_organization(cleaned):
            return ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        
        # –û—Ç—Ä–µ–∑–∞–µ–º –¢–û–õ–¨–ö–û –≥–æ—Ä–æ–¥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ " –ì. –ì–æ—Ä–æ–¥" (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è)
        separate_city_patterns = [
            r'\s+[–ì–≥Gg]\.?\s+[–ê-–Ø–Å][–∞-—è—ë\-]+(?:\s*,.*)?$',  # " –ì. –ò—Ä–∫—É—Ç—Å–∫" –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
            r'\s+–≥–æ—Ä–æ–¥\s+[–ê-–Ø–Å][–∞-—è—ë\-]+(?:\s*,.*)?$',      # " –≥–æ—Ä–æ–¥ –ë–∞—Ä–Ω–∞—É–ª" –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
        ]
        
        for pattern in separate_city_patterns:
            match = re.search(pattern, cleaned)
            if match:
                # –û—Ç—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –≥–æ—Ä–æ–¥ –≤ –∫–æ–Ω—Ü–µ
                cleaned = cleaned[:match.start()].strip()
                break
        
        # –£–¥–∞–ª—è–µ–º –¢–û–õ–¨–ö–û —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏ –Ω–æ–º–µ—Ä–∞ (–ù–ï –≥–µ–æ–≥—Ä–∞—Ñ–∏—é!)
        cleaned = re.sub(r'\s+(–¢–µ–ª\.?|–§–∞–∫—Å|Email|E-mail|–ú–æ–±\.?)(\s|$)', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\d{1,2}-\d{4}-\d{2}-\d{2}', '', cleaned)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤
        if self._is_stop_word_org(cleaned):
            return ""
        
        return cleaned.strip()

    def merge_person_fragments(self, person_fragments: List[str], text: str) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –§–ò–û —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"""
        merged_names = []
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –§–ò–û
        fio_patterns = [
            r'([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+)',  # –§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ
            r'([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.)',            # –§–∞–º–∏–ª–∏—è –ò. –û.
        ]
        
        for pattern in fio_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –§–ò–û
                full_name = self._clean_fio_text(match.strip())
                if full_name and not self._is_stop_word_person(full_name):
                    merged_names.append(full_name)
        
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        if not merged_names and len(person_fragments) >= 1:
            for fragment in person_fragments:
                clean_fragment = self._clean_fio_text(fragment)
                if clean_fragment and not self._is_stop_word_person(clean_fragment):
                    merged_names.append(clean_fragment)
        
        return list(set(merged_names))

    def extract_full_addresses(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å–∞ –∏ –≥–æ—Ä–æ–¥–∞"""
        addresses = []
        
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # –ò—â–µ–º –ª—é–±—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å –∞–¥—Ä–µ—Å–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏
            address_markers = ['–≥.', '—É–ª.', '–¥–æ–º', '–¥.', '—É–ª–∏—Ü–∞', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–ø—Ä.', '–∫–æ—Ä–ø—É—Å', '–∫–æ—Ä.', '–æ—Ñ–∏—Å', '–æ—Ñ.', '–∫–≤–∞—Ä—Ç–∏—Ä–∞', '–∫–≤.']
            
            if any(marker in line.lower() for marker in address_markers):
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
                cleaned_line = re.sub(r'^\W+|\W+$', '', line)
                if len(cleaned_line) > 8:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–¥—Ä–µ—Å–∞
                    addresses.append(cleaned_line)
            
            # –û—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≥–æ—Ä–æ–¥–æ–≤ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
            elif re.match(r'^[–ì–≥Gg]\.?\s*([–ê-–Ø–Å][–∞-—è—ë\-]+)$', line):
                # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "–ì. –ò—Ä–∫—É—Ç—Å–∫" –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                city_match = re.search(r'[–ì–≥Gg]\.?\s*([–ê-–Ø–Å][–∞-—è—ë\-]+)', line)
                if city_match:
                    addresses.append(f"–≥. {city_match.group(1)}")
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∞–¥—Ä–µ—Å–∞, –∏—â–µ–º –ø—Ä–æ—Å—Ç—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤
        if not addresses:
            for line in lines:
                line = line.strip()
                if re.match(r'^[–ê-–Ø–Å][–∞-—è—ë\-]+$', line) and len(line) > 3:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å—é –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
                    line_idx = lines.index(line)
                    is_city = True
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏
                    for check_idx in [line_idx - 1, line_idx + 1]:
                        if 0 <= check_idx < len(lines):
                            check_line = lines[check_idx].strip()
                            if any(word in check_line.lower() for word in ['–≤—Ä–∞—á', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è', '–º–µ—Ç—Ä–æ–ª–æ–≥–∏–∏']):
                                is_city = False
                                break
                    
                    if is_city:
                        addresses.append(line)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–ø–æ–ª–Ω—ã–µ –∞–¥—Ä–µ—Å–∞ –≤–∞–∂–Ω–µ–µ)
        if addresses:
            addresses = sorted(set(addresses), key=len, reverse=True)
        
        return addresses

    def extract_clean_positions(self, text: str) -> List[str]:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø: –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        positions = []
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ —Å—Ç—Ä–æ–∫–∏ —Å –§–ò–û
            if not line or re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\s*[–ê-–Ø–Å]*[–∞-—è—ë]*\s*$', line):
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
            if re.search(r'\b(–û–û–û|–ê–û|–ó–ê–û|–ò–ü|–§–ì–ë–û–£|–§–ë–£–ó)\b', line, re.IGNORECASE):
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∞–¥—Ä–µ—Å–∞–º–∏ –∏ –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏
            if re.search(r'\b(–≥\.|—É–ª\.|—Ç–µ–ª\.?|email|—Ñ–∞–∫—Å|\+7|\d{1,2}-\d{4})\b', line, re.IGNORECASE):
                continue
            
            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –¥–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
            line_lower = line.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            starts_with_position = False
            for keyword in self.position_keywords:
                if line_lower.startswith(keyword):
                    starts_with_position = True
                    break
            if starts_with_position:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω—É—é –¥–ª–∏–Ω—É (–Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è –¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
                if len(line) < 150:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ª–∏–º–∏—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
                    clean_position = self.clean_position_text(line, line)
                    if clean_position and clean_position not in positions:
                        positions.append(clean_position)
    
        return positions

    def clean_position_text(self, position: str, full_line: str = "") -> str:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø: –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä—å"""
        position = re.sub(r'\s+', ' ', position).strip()
        
        if not position:
            return ""
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        position = re.sub(r'\d{1,2}-\d{4}-\d{2}-\d{2}', '', position)
        position = re.sub(r'\b(–¢–µ–ª\.?|–§–∞–∫—Å|Email|E-mail)\b.*', '', position, flags=re.IGNORECASE)
        position = re.sub(r'^[^\w]+|[^\w]+$', '', position)
        position = re.sub(r'\s+', ' ', position).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        if not any(keyword in position.lower() for keyword in self.position_keywords):
            return ""
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
        words = position.split()
        if words:
            # –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å
            result = words[0].capitalize()
            if len(words) > 1:
                result += ' ' + ' '.join(words[1:])
            return result
        
        return position.strip()

    def extract_entities(self, text: str) -> NERResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å—Ç–æ–ø-—Å–ª–æ–≤"""
        
        result = NERResult()
        
        # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç Natasha
        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)
        doc.parse_syntax(self.syntax_parser)
        doc.tag_ner(self.ner_tagger)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç Natasha
        raw_persons = []
        raw_orgs = []
        raw_locations = []
        
        for span in doc.spans:
            span.normalize(self.morph_vocab)
            
            if span.type == PER:
                raw_persons.append(span.text)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º span.text –≤–º–µ—Å—Ç–æ span.normal
            elif span.type == ORG:
                raw_orgs.append(span.text)     # –ò—Å–ø–æ–ª—å–∑—É–µ–º span.text –≤–º–µ—Å—Ç–æ span.normal
            elif span.type == LOC:
                raw_locations.append(span.text)
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å—Ç–æ–ø-—Å–ª–æ–≤
        result.persons = self.merge_person_fragments(raw_persons, text)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        filtered_orgs = []
        for org_text in raw_orgs:
            org_str = org_text.strip()
            
            # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –∏–∑ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            words = org_str.split()
            if words and words[0].lower() in self.position_keywords:
                org_str = ' '.join(words[1:])
            
            if not org_str:
                continue
            
            # –ù–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é —ç—Ç—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            line = next((l for l in text.split('\n') if org_str in l), org_str)
            full_org = self._extend_org_with_region(org_str, line)
            
            # –û—á–∏—â–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é (–≤–∫–ª—é—á–∞—è –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
            clean_org = self.clean_organization_text(full_org)
            if clean_org and clean_org not in filtered_orgs:
                filtered_orgs.append(clean_org)
        
        result.organizations = filtered_orgs
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–¥—Ä–µ—Å–æ–≤ –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        result.locations = self.extract_full_addresses(text)
        result.positions = self.extract_clean_positions(text)
        
        return result

    def extract_city_from_address(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥ –∏–∑ –∞–¥—Ä–µ—Å–∞"""
        
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # –ò—â–µ–º –≥–æ—Ä–æ–¥–∞ –≤ –ª—é–±—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö —Å –∞–¥—Ä–µ—Å–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏
            if any(marker in line.lower() for marker in ['–≥.', '–≥–æ—Ä–æ–¥']):
                city_patterns = [
                    r'[–ì–≥Gg]\.?\s*([–ê-–Ø–Å][–∞-—è—ë\-]+)',  # –≥. –ë–∞—Ä–Ω–∞—É–ª
                    r'–≥–æ—Ä–æ–¥\s+([–ê-–Ø–Å][–∞-—è—ë\-]+)',      # –≥–æ—Ä–æ–¥ –ë–∞—Ä–Ω–∞—É–ª
                ]
                
                for pattern in city_patterns:
                    match = re.search(pattern, line)
                    if match:
                        return match.group(1)
        
        # –ü–æ–∏—Å–∫ –≥–æ—Ä–æ–¥–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        for line in lines:
            line = line.strip()
            if re.match(r'^[–ì–≥Gg]\.?\s*[–ê-–Ø–Å][–∞-—è—ë\-]+$', line):
                city_match = re.search(r'[–ì–≥Gg]\.?\s*([–ê-–Ø–Å][–∞-—è—ë\-]+)', line)
                if city_match:
                    return city_match.group(1)
            elif re.match(r'^[–ê-–Ø–Å][–∞-—è—ë\-]+$', line) and len(line) > 3:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å—é –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
                line_idx = lines.index(line)
                is_city = True
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏
                for check_idx in [line_idx - 1, line_idx + 1]:
                    if 0 <= check_idx < len(lines):
                        check_line = lines[check_idx].strip()
                        if any(word in check_line.lower() for word in ['–≤—Ä–∞—á', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–æ—Ç–¥–µ–ª–µ–Ω–∏—è', '–º–µ—Ç—Ä–æ–ª–æ–≥–∏–∏']):
                            is_city = False
                            break
                
                if is_city:
                    return line
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –ª–æ–∫–∞—Ü–∏–π, –µ—Å–ª–∏ –µ—Å—Ç—å
        result = self.extract_entities(text)
        if result.locations:
            first_address = result.locations[0]
            city_match = re.search(r'[–ì–≥Gg]\.?\s*([–ê-–Ø–Å][–∞-—è—ë\-]+)', first_address)
            if city_match:
                return city_match.group(1)
            return first_address.split(',')[0].strip()
        
        return None

# –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –ª–æ–≥–æ–º
def test_ner_extractor():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ NER-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    
    print("=== üß™ –¢–ï–°–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û NER –≠–ö–°–¢–†–ê–ö–¢–û–†–ê ===")
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Natasha...")
    
    extractor = RussianNERExtractor()
    
    # –í—Å–µ —Ç–µ—Å—Ç—ã –≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–µ
    test_signatures = [
        {
            'name': 'üî¨ –°–∏–±–õ–∞–±–°–µ—Ä–≤–∏—Å',
            'text': """–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–ö–ª–µ–±–∞–Ω–æ–≤–∞ –ò—Ä–∏–Ω–∞ –î–º–∏—Ç—Ä–∏–µ–≤–Ω–∞
–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –±–∏–∑–Ω–µ—Å–∞
–¢–µ–ª.(3952)78-25-79, –¥–æ–ø. 121
–ó–ê–û ¬´–°–∏–±–õ–∞–±–°–µ—Ä–≤–∏—Å¬ª
–ì. –ò—Ä–∫—É—Ç—Å–∫, —É–ª. –°—Ç–∞–ª–∏–Ω–∞, –¥.7, –∫–≤. 104"""
        },
        {
            'name': '‚öïÔ∏è –ê–ª—Ç–∞–π—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä',
            'text': """–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–•–∞—Ä–ª–æ–≤–∞ –û–ª–µ—Å—è –ê–Ω–∞—Ç–æ–ª—å–µ–≤–Ω–∞
–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—Å–∞ –ê–ª—Ç–∞–π—Å–∫–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–π –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
–§–ë–£–ó ¬´–¶–µ–Ω—Ç—Ä –≥–∏–≥–∏–µ–Ω—ã –∏ —ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏–∏ –≤ –ê–ª—Ç–∞–π—Å–∫–æ–º –∫—Ä–∞–µ¬ª
–≥. –ë–∞—Ä–Ω–∞—É–ª, –°–µ–º–∫–∏–Ω–∞, 1–∞, –∫–æ—Ä. 2, –æ—Ñ. 310
8-8182-23-45-38"""
        },
        {
            'name': 'üèõÔ∏è –†–µ—Å–ø—É–±–ª–∏–∫–∞ –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω',
            'text': """–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á
–ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤ —Ä–∞–∑–≤–∏—Ç–∏—è –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –≤ —Ä. –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω
–û–û–û "–†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞" –≤ –†–µ—Å–ø—É–±–ª–∏–∫–µ –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω
–≥. –ö–∞–∑–∞–Ω—å, –ö—Ä–µ–º–ª—ë–≤—Å–∫–∞—è —É–ª., –¥.2, –æ—Ñ.5
+7 (843) 123-45-67"""
        },
        {
            'name': 'üå≤ –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–∞—è –æ–±–ª.',
            'text': """–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–ü–µ—Ç—Ä–æ–≤ –ü–µ—Ç—Ä –ü–µ—Ç—Ä–æ–≤–∏—á
–î–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –Ω–∞–π–º—É
–ê–û "–°–µ–≤–µ—Ä–Ω–∞—è –ó–≤–µ–∑–¥–∞" –≤ –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–æ–π –æ–±–ª.
–≥. –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫, —É–ª. –õ–µ–Ω–∏–Ω–∞, –¥.10, –∫–≤.20
8-8182-23-45-67"""
        }
    ]
    
    for test in test_signatures:
        print(f"\n{'='*60}")
        print(f"üìß –¢–ï–°–¢: {test['name']}")
        print(f"{'='*60}")
        print(f"üìù –¢–µ—Å—Ç–æ–≤–∞—è –ø–æ–¥–ø–∏—Å—å:\n{test['text']}")
        
        result = extractor.extract_entities(test['text'])
        city = extractor.extract_city_from_address(test['text'])
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"üë§ –ü–µ—Ä—Å–æ–Ω—ã: {result.persons}")
        print(f"üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {result.organizations}")
        print(f"üíº –î–æ–ª–∂–Ω–æ—Å—Ç–∏: {result.positions}")
        print(f"üìç –õ–æ–∫–∞—Ü–∏–∏ (–ê–¥—Ä–µ—Å–∞): {result.locations}")
        print(f"üèôÔ∏è –ì–æ—Ä–æ–¥: {city}")

if __name__ == "__main__":
    test_ner_extractor()
